\documentclass{article}
\usepackage{amsmath} % For math equations
\usepackage{amsfonts} % For math fonts
\usepackage{amssymb} % For math symbols
\usepackage{float}
\usepackage{enumitem}
\usepackage{graphicx}
\setlist[enumerate,1]{label=\arabic*.}
\setlist[enumerate,2]{label=\alph*.,itemindent=2em}

\title{HW 3 - 115B}
\author{Asher Christian 006-150-286}
\date{ 24.01.25}

\begin{document}
    \maketitle
    \section{Exercise 1}
    \emph{
        Let $A$ be a $2 \times 2$ diagonalizable matrix. Prove the statement
        of Cayley-Hamilton theorem directly, using the fact that $A = QDQ^{-1}$
        for some invertible  $Q \in k^{2 \times 2}$ and some diagonal $D \in k^{2 \times 2}$
    }
    First we show that it is true for $D$ 
    \[
        D = \begin{pmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{pmatrix} 
    .\] 
    \[
    P_D(t) = (\lambda_1-t)(\lambda_2-t)
    .\] 
    \[
        P_D(D) = (\begin{pmatrix} 0 && 0 \\ 0 && \lambda_1-\lambda_2 \end{pmatrix} )(\begin{pmatrix} \lambda_2-\lambda_1 & 0 \\ 0 & 0 \end{pmatrix}  = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} 
    .\] 
    This holds for any $\lambda_1, \lambda_2 \in k$ Now we use determinant rules
    \[
    \det(D-tI) = \det(Q(D-tI)Q^{-1}) = \det((QD-Q(tI))Q^{-1}) = \det(QDQ^{-1} - t(QIQ^{-1}))
    .\] 
    \[
    = \det(A - tI)
    .\] 
    So the two matrices have the same characteristic polynomial.
    Additionally
    \[
    P_D(A) = P_D(QDQ^{-1}) = QP_D(D)Q^{-1} = Q0Q^{-1} = 0
    .\] 
    The fact used here is that for each term $a_it^{i}$ 
    \[
    a_i(QDQ^{-1})^{i} = a_i(QD^{i}Q^{-1}) = Q(a_iD^{i})Q^{-1}
    .\] 
    This holds for every element of the polynomial and the $Q$ and $Q^{-1}$
    can be factored out by the distributive property.
    So we have shown that the characteristic polynomial annihilates $A$

    \section{Exercise 2 / 3}
    \emph{
        For each linear endomorphism $T$ on the vector space
        $V$ find an ordered basis for the $T$-cyclic subspace generated by the vector
        $\vec{v}$ .\\
        Additionally Compute the characteristic polynomial of $T|_W$
    }
    \begin{enumerate}
        \item $V = \mathbb{R}^{4}$, $T(\begin{pmatrix} w \\ x \\ y \\ z \end{pmatrix} ) = \begin{pmatrix} w + x \\ x-y \\ w+y\\ w+z \end{pmatrix} , \vec{v} = \vec{e_1}$\\
            Listing out elements generated
            \[
            \vec{v_1} = \begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \end{pmatrix} ,\vec{v_2} =  \begin{pmatrix} 1 \\ 0 \\ 1 \\ 1 \end{pmatrix} , \vec{v_3} = \begin{pmatrix} 1 \\ -1 \\ 2 \\ 2 \end{pmatrix},\vec{v_4} =  \begin{pmatrix} 0 \\ -3 \\ 3 \\ 3 \end{pmatrix} 
            .\] 
            But $\vec{v_4} = -3\vec{v_2} + 3\vec{v_3}$ so only the first three are
            linearly independent and from the theorem presented in lecture we may stop with the first three
            vectors that are linearly independent and they form a basis\\
            Using the basis as described before
            \[
                [T|_W]_B = \begin{pmatrix} 0 & 0 & 0\\
                    1 & 0 & -3\\
                    0 & 1 & 3 \\
                \end{pmatrix} 
            .\] 
            With
            \[
            \det(T_W - \lambda I) = 
            \det( \begin{pmatrix} -\lambda & 0 & 0\\
            1 & -\lambda & -3\\
        0 & 1 & 3-\lambda\end{pmatrix} 
            .\] 
            \[
            = -\lambda(-\lambda(3-\lambda) +3) = \lambda(-\lambda^2+3\lambda-3)
            .\] 
        \item $V = \mathbb{R}[x]_{\le 3}, \; T(f(x)) = f''(x), \vec{v} = x^{2}$ 
            Listing elements
            \[
            \vec{v_1} = x^2, \vec{v_2} = 2, \vec{v_3} = 0
            .\] 
            And so a basis is $\vec{v_1}$ and $\vec{v_2}$\\
            Using the basis 
            \[
                [T|_W]_B = \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix} 
            .\] 
            We get the characteristic polynomial
            \[
            \det(T-\lambda I) = \lambda^2
            .\] 
        \item $V = k^{2 \times 2}, \; T(A) = A^{T}, \; \vec{v} = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} $
            Listing elements
            \[
                \vec{v_1} = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} , \vec{v_2} = \begin{pmatrix} 0 &  1 \\ 1 & 0 \end{pmatrix} 
            .\] 
            This is a linearly dependent set so only the first vector is considered
            and by itself it forms a basis of the T-cyclic subspace.
            \\
            The characteristic polynomial associated with this matrix is
            \[
            P_T(\lambda) = (1-\lambda)
            .\] 
        \item $V = k^{2 \times 2}, \; T(A) = L_{\begin{pmatrix} 1 & 1 \\ 2 & 2 \end{pmatrix} }(A), \; \vec{v} = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} $ \\
            Listing elements
            \[
                \vec{v_1} = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} , \vec{v_2} = \begin{pmatrix} 1 & 1 \\ 2 & 2 \end{pmatrix}, \vec{v_3} = \begin{pmatrix} 3 & 3 \\ 6 & 6 \end{pmatrix} 
            .\] 
            This set is linearly dependent  since $\vec{v_3} = 3 \vec{v_2}$ 
            so the first two vectors form a basis
            Using this basis a matrix representation is as follows
            \[
                [T|_W]_B = \begin{pmatrix} 0 & 0 \\ 1 & 3 \end{pmatrix} 
            .\] 
            With characteristic polynomial
            \[
            P_T(\lambda) = (-\lambda)(3-\lambda)
            .\] 

    \end{enumerate}
    \section{Exercise 4}
    \emph{
        Let $V$ and $W$ be non-zero finite dimensional $k$-vector spaces and let
        \[
        T: V \rightarrow W
        .\] be a linear transformation
        \begin{enumerate}
            \item Prove that $T$ is onto if and only if $T^{*}$ is one-one\\
                \emph{
                    For the first direction if $T^{*}$ is one-one assume for
                    contradiction that $T$ is not onto. Then there exists some $w \in W$ s.t.
                    $Tv \ne w_0$ for all $v \in V$. Since $W$ is finite dimensional pick a basis
                    for $W$  $B = \{w_0, w_1,w_2,...,w_n\}$ with $w_0$ as before.
                    Consider the element  $f \in W^*$ defined on the basis as
                    \[
                    f(w_i) =
                    \begin{cases}
                        1 & i = 0\\
                        0 & i \ne 0
                    \end{cases}
                    .\] 
                    Then for any $v \in V$,
                    \[
                    T^{*}(f)(v) = f(Tv) = 0
                    .\] 
                    This implies that $T^{*}(f) = 0 \in V^{*}$ but this is a contradiction since 
                    we assumed that $T^*$ is one-one since $T^{*}(0) = 0$ as well. \\
                    To prove the second direction, if $T$ is onto assume for contradiction that $T^*$ is not one-one
                    Then there exists some $f_1,f_2 \in W^{*}, \;\; f_1 \ne f_2$ such that
                    \begin{align*}
                        T^{*}(f_1) &= T^{*}(f_2)\\
                        f_1(Tv) &= f_2(Tv)
                    \end{align*}
                    For all $v \in V$.
                    But since $f_1 \ne f_2$ there exists some $w \in W$ such that $f_1(w) \ne f_2(w)$
                    and by $T$ onto we have there exists some $v \in V$ such that $T(v) = w$. This is a contradiction
                    because we have two $v$ that agree
                }
            \item Prove that $T^{*}$ is onto iff $T$ is one-one\\
                \emph{
                    To prove the first direction, if $T$ is one-one pick a basis $ \{v_1,v_2,...,v_n\}$ for $V$.
                    Then since $T$ is one-one the set $\{Tv_1,Tv_2,...,Tv_n\}$ is linearly independent and extend it to a
                    basis $\{Tv_1,...,Tv_n,w_1,...,w_m\}$ for $W$
                    Let  $f \in V^{*}$. THen
                    \[
                    f = \alpha_1 v_1^{*} + ... + a_nv_n^{*}
                    .\] 
                    with each $\alpha_i \in k$
                    where $v_i^{*}$ is the linearly functional defined on the basis taking $v_i $ to one and all other
                    basis vectors to 0.
                    Then pick $g \in W^{*}$ such that
                     \[
                    g = \alpha_1(Tv_1)^{*} + ... + \alpha_n(Tv_n)^{*}
                    .\] 
                    Then
                    \[
                    T^{*}g(v_i) = g(Tv_i) = \alpha_i = f(v_i)
                    .\] 
                    and so the linear functionals agree on a basis and thus agree so we have shown
                    that every $f$ is in the form $T^{*}g$ for some $g \in W^{*}$.\\
                    For the second direction if $T^{*}$ is onto assume for contradiction that $T$ is not one-one
                    Then there exists
                    \[
                    v_1,v_2 \in V \;\; v_1 \ne v_2 \;\; Tv_1=Tv_2
                    .\] 
                    since $v_1 \ne v_2$ there exists some $f \in V^{*}$ such that $f(v_1) \ne f(v_2)$
                    but since $T^{*}$ is onto there exists some $g \in W^{*}$ such that $T^{*}g = f$
                    \[
                    g(Tv_1) = f(v_1) = \alpha = g(w)
                    .\] 
                    \[
                    g(Tv_2) = f(v_2) = \beta = g(w)
                    .\] 
                    so $g(w) = \alpha$ and $g(w) = \beta$ but $\beta \ne \alpha$ a contradiction thus $T$ is one-one
                }
        \end{enumerate}
    }
    \section{Exercise 5}
    \emph{Fix some $d \in \mathbb{Z}^{\ge 1}$ and some scalars $a_0,...,a_{d-1} \in k$ let $A$ 
        denote the $d \times d$ matrix
        \[
            \begin{pmatrix} 0 & 0 & ... & 0 & -a_0 \\
                1 & 0 & ... & 0 & -a_1\\
                0 & 1 & ... & 0 & -a_2\\
                \vdots & \vdots & \ddots & \vdots & \vdots\\
                0 & 0 & ... & 0 & -a_{d-2}\\
                0 & 0 & ... & 1 & -a_{d-1}
            \end{pmatrix}
        .\] 
    Prove that the characteristic polynomial of $A$ is $(-1)^{d}(a_0+a_1t+...+a_{k-1}t^{d-1} + t^{d}$
    }
    First proving the base case $d = 2$
     \[
         \det( \begin{pmatrix} 0 & -a_0 \\ 1 & -a_1 \end{pmatrix} - tI) = \det( \begin{pmatrix} -t & -a_0 \\ 1 & -a_1 - t \end{pmatrix} ) = (-1)^2(a_0+a_1t+t^2)
    .\] 
    Now assume it holds for $d < n$, then for the $n \times n$ matrix
    \[
    \det( \begin{pmatrix} 
        -t & 0 & ... & 0 & -a_0\\
        1 & -t & ... & 0 & -a_1\\
        0 & 1 & ... & 0 & -a_2\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
        0 & 0 & ... & -t & -a_{n-2}\\
        0 & 0 & ... & 1 & -a_{n-1} - t
    \end{pmatrix} 
    \]
    \[
    = (-t)(\det( \begin{pmatrix} 
         -t & ... & 0 & -a_1\\
         1 & ... & 0 & -a_2\\
         \vdots & \ddots & \vdots & \vdots\\
         0 & ... & -t & -a_{n-2}\\
         0 & ... & 1 & -a_{n-1} - t
    \end{pmatrix} ) + (-1)^{n-1}(-a_0)\det( \begin{pmatrix} 
        1 & -t & ... & 0 \\
        0 & 1 & ... & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & ... & -t \\
        0 & 0 & ... & 1 
     \end{pmatrix} 
    .\] 
    \[
        = (-t)((-1)^{n-1})(a_1+a_2t+...+a_{n-1}t^{n-2} + t^{n-1})) + (-1)^{n}(a_0)
    .\] 
    \[
        = (-1)^{n}(a_0+a_1t+...+a_{n-1}t^{n-1}+ t^{n}
    .\] 
    Proving the iterative step, so by induction we have shown the relationship holds.
    \section{Exercise 6}\emph{ Let $T$ be a linear endomorphism of a finite dimensional vector
    space $V$.}
    \begin{enumerate}
        \item \emph{Prove that if the characteristic polynomial of $T$ spl,its, then so does the characteristic polynomial
            fo the restriction of $T$ to any $T$-invariant subspace of $V$.}\\
                Using the definition that splitting refers to being the product of linear factors it suffices to show that
                $f_{T|_W} | f_{T}$ when $W$ is $T$-invariant.
                Suppose that $W$ is $T$ invariant and consider any basis $B_1 = \{w_1,w_2,...,w_n\}$ of  $W$ since $W$ is finite dimensional.
                Extend that basis to $B_2 = \{w_1,w_2,...,w_n,v_1,...,v_d\}$ of $V$ again since finite dimensional. 
                Consider the representation
                \[
                    [T]_{B_2} = \begin{pmatrix} 
                        [T|_W]_{B_1} & A_1 \\
                        0 & A_2
                    \end{pmatrix} 
                .\] 
                where $A_1$ and $A_2$ are block matrices. and so
                \[
                    [T-tI]_{B_2} = \begin{pmatrix}  
                        [T|_W-tI]_{B_1} & A_1\\
                        0 & A_2-tI
                    \end{pmatrix} 
                .\] 
                And By the rules of taking determinants of block matrics
                \[
                    f_T = f_{T|_W}\det(A_2-tI)
                .\] 
                and so $f_{T|_W}$ divides the characteristic polynomial and in particular if 
                the characteristic polynomial splits so does the restriction to any $T$ invariant subspace.
            \item \emph{
                    Deduce that if the characteristic polynomial of $T$ splits, then any nonzero $T$-invariant subspace of $V$ 
                    contains an eigenvector of  $T$\\
                }
                Consider the characteristic polynomial of the restriction $f_{T|_W}$ with $W$ being the $T-$invariant subspace.
                By the previous part of the question, $f_{T|_W}$ splilts and is of positive degree. In particular there exists a factor
                \[
                    (\alpha-t)
                .\] 
                for some $\alpha$ in the base field. We use the fact that since $\alpha$ is a root of the characteristic polynomial
                it implies that the transformation  $T|_{W} - \alpha I$ is not invertible and so there exists some $v \in W$ with $v \ne 0$ such that
                $(T|_W - \alpha I)v = 0$ which is equivalent to $T|_W v = \alpha v$ thus $W$ has an eigenvector of $T|_W$ but
                by definition  $T|_W v = Tv$ since $v \in W$ and so  $Tv = \alpha v$ and $v$ is an eigenvector of $T$
    \end{enumerate}
    \emph{
    Let $T$ be a linear operator on a finite dimensional vector space $V$, and let $W$ be a $T$-invariant subspace of $V$}
    \begin{enumerate}
        \item\emph{ suppose that $v_1,v_2,...,v_d$ are eigenvectors of $T$ corresponding to distinct eigenvalues.
            Prove that if $\sum_{i=1}^{d}v_i$ is in $W$, then $v_i \in W$ for all $i \in \{1,2,...,d\}$ \\}
                We prove inductively a stronger argument that if $\sum_{i=1}^{d}\beta_iv_i \in W$ then each $v_i \in W$ with $\beta_i \ne 0$ for all $i$. For the base case if $d = 1$ the statement is trivially true after 
                dividing the single term by $\beta_1$.
                Assume now that the statement holds for $d = n$ then for $d = n+1$ we have
                \begin{align*}
                    v &= \beta_1v_1+\beta_2v_2+...+\beta_nv_n+\beta_{n+1}v_{n+1}\\
                    Tv &= \alpha_1\beta_1v_1 + \alpha_2\beta_2v_2 + ... +\alpha_n\beta_nv_n + \alpha_{n+1}\beta_{n+1}v_{n+1}\\
                \end{align*}
                Consider the element
                \[
                    \alpha_{n+1}v - Tv = \beta_1(\alpha_{n+1}-\alpha_1)v_1 + \beta_2(\alpha_{n+1}-\alpha_2)v_2 + ... + \beta_n(\alpha_{n+1}-\alpha_n)v_n + \beta_{n+1}(\alpha_{n+1}-\alpha_{n+1})v_{n+1}
                .\] 
                the coefficient for element $v_i$ is
                \[
                    \beta_i(\alpha_{n+1}-\alpha_i)
                .\] 
                which is not zero if $i \ne n+1$ since all eigenvalues are distinct and so this vector
                 \[
                     w = a_{n+1}v - Tv \in W
                .\] 
                and by the inductive hypothesis this implies that each $v_1,v_2,...,v_n$ is in $W$.
                But since each of these vectors is in  $W$ we can isolate the $v_{n+1}$ term in the original vector by the equation
                \[
                    v_{n+1} = \frac{1}{\beta_{n+1}}(v - \sum_{i=1}^{n}\beta_iv_i)
                .\] 
                and so  $v_{n+1}$ is in $W$ proving the inductive step. in particular picking each $\beta_i = 1$ we get the statement of the original
                question.
            \item \emph{
                    Suppose that $\dim(V) = n$ and  $T$ has $n$ distinct eigenvalues. Prove that
                    $V$ itself is a $T$-cyclic subspace. 
                }\\
                Consider the vector $v = \sum_{i=1}^{n}v_i$ with each $v_i$ and eigenvector which in a previous problem we showed could be found for each distinct eigenvalue and let $W$ be the $T$-cyclic subspace
                generated by  $v$. By the previous question each $v_i \in W$.  The set $\{v_1,v_2,...,v_n\}$ is
                linearly independent because assume for contradiction they were not then without loss of generality let $v_1$ be a linear combination of the other vectors
                \[
                v_1 = \beta_2v_2 + \beta_3v_3 +... + \beta_nv_n
                .\] 
                \[
                Tv_1 = \beta_2\alpha_2v_2+ \beta_3\alpha_3v_3 + ... + \beta_n\alpha_nv_n \ne \alpha_1v_1
                .\] 
                since the eigenvalues are distinct. And since $W$ is a subspace of  $V$ its dimension is
                less than or equal to $n$, however since $W$ has $n$ linearly independent vectors it must have dimension at
                least $n$, and so its dimension will be exactly $n$ and since $v_1,v_2,...,v_n$ are $n$ linearly independent vectors in $W$ they
                form a basis. and since $\dim(W) = \dim(V)$  $W = T$. So  $V$ is a $T$-cyclic subspace generated by $v$.
                
                
    \end{enumerate}
    \section{Exercise 8}
    \emph{
        Prove that the restriction of a diagnoalizable linear operator $T$ to any non-trivial $T$-invariant subspace
        is also diagonalizable
    }\\
    Let $T$ be a diagnoalizable linear operator, then there exists some basis where every element is an eigenvector.
    Assume for contradiction that $W$ is a $T$-invariant subspace but $T|_W$ is not diagnoalizable, then there is some vector $v \in W$
    such that $v$ can not be represented as the sum of linearly independent eigenvectors.
    Consider the representation $v = \sum_{i \in I}^{}\alpha_iv_i$ of $v$ in $V$ where each  $v_i$ is an eigenvector under the transformation $T$. 
    This is possible because $T$ is diagonalizable. This sum must be finite and so by the previous question, grouping elements of equal eigenvalue there exist vectors
    corresponding to each eigenvalue in $W$, the span of which includes $v$ a contradiction and so $W$ is spanned by eigenvectors under $T$ and $T|_W$ is diagonalizable.
    \\
    In particular if each eigenvalue is distinct we are done and if not for any eigenvalue let $\{w_1,w_2,...,w_d\}$ be the eigenvectors  corresponding
    to that eigenvalue, if $W$ has any linear combination of such vectors they can be grouped into sums of vectors with the same eigenvalue and whichever subset is in $W$ can be represented
    as a unique basis element.
    
    \section{Exercise 9}
    \emph{
        Let $A \in k^{n \times n}$ for some $n \in \mathbb{Z}^{\ge 0}$. Prove that $\dim(span\{I_n,A,A^2,A^{3},...\}) \le n$
    }
    consider the characteristic polynomial  $f_A(t)$.  $\deg(f_A) \le n$ in particular if
     \[
    f_A(t) = a_0 + a_1t + ... + (-1)^{n}t^{n}
    .\] 
    then
    \[
    f_A(A) = a_0I_n + a_1A + a_2A^2 + ... + (-1)^{n}A^{n} = 0
    .\] 
    so $A^{n}$ is in the span of $\{I_n,A,A^2,...,A^{n-1}\}$. And so if $T: k^{n \times n} \rightarrow k^{n \times n}$ defined
    by left multiplication by $A$ the set described is a $T$-cyclic subspace generated by $I_n$ 
    and so by identical argument as described in class it contains no more than $n$ elements since $A^{n}$ is a linear combination of
    the other elements. Thus the dimension of the span is less than or equal to $n$.


\end{document}
