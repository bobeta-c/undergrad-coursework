\documentclass[9pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, multicol, geometry, enumitem}
\geometry{a4paper, margin=0.5in}
\setlength{\parskip}{0pt}
\setlength{\itemsep}{1pt}
\title{Numerical Methods Cheat Sheet}
\author{}
\date{March 19, 2025}
\begin{document}
\maketitle
\begin{multicols}{2}

\section*{16: Numerical Differentiation}
\textbf{Goal:} Approximate $f'(x)$ when analytic $f'(x)$ is hard. \\
\textbf{Applications:} ODEs like $\frac{df}{dt} = \alpha f\left(1-\frac{f}{k}\right)$. \\
\textbf{Methods:} \\
- \textit{Forward Diff.:} $f'(x_0) \approx \frac{f(x_0+h)-f(x_0)}{h}$, Error $\approx \frac{h}{2}M$, $M = \max|f''(x)|$ \\
- \textit{Backward Diff.:} $f'(x_0) \approx \frac{f(x_0)-f(x_0-h)}{h}$, Error $\approx \frac{h}{2}M$ \\
- \textit{Central Diff.:} $f'(x_0) \approx \frac{f(x_0+h)-f(x_0-h)}{2h}$, Error $\approx \frac{h^2}{6}f'''(\zeta)$ \\
\textbf{Ex:} $f'(1.8)$ for $f(x) = \ln x$, $h=0.1$: $\approx 0.5406722$, Error $< 0.0154321$.

\section*{17: Round-Off \& Richardson}
\textbf{Round-Off Error:} \\
- Central Diff.: $\left|f'(x_0) - \frac{\bar{f}(x_0+h)-\bar{f}(x_0-h)}{2h}\right| \leq \frac{\varepsilon}{h} + \frac{h^2}{6}M$ \\
\textbf{Richardson Extrapolation:} \\
- $D^+_hf(x_0) = \frac{f(x_0+h)-f(x_0)}{h}$ \\
- 2nd-Order: $f'(x_0) = 2D^+_{\frac{h}{2}}f(x_0) - D^+_hf(x_0) + \mathcal{O}(h^2)$ \\
- General: $M = 2N\left(\frac{h}{2}\right) - N(h) + \mathcal{O}(h^2)$, $M = 4N\left(\frac{h}{2}\right) - N(h)/3 + \mathcal{O}(h^4)$

\section*{18: Numerical Integration}
\textbf{Goal:} Approximate $\int_a^b f(x)dx$ (e.g., $\int_{-1}^1 e^{-x^2}dx$). \\
\textbf{Trapezoidal Rule:} \\
- $\int_a^b f(x)dx \approx \frac{h}{2}(f(a)+f(b)) - \frac{h^3}{12}f''(c)$, $h = b-a$ \\
- 2nd-order, Degree of Precision: 1 \\
\textbf{Simpson’s Rule:} \\
- $\int_a^b f(x)dx \approx \frac{h}{3}(f(x_0)+4f(x_1)+f(x_2)) - \frac{h^5}{90}f^{(4)}(\eta)$ \\
- 4th-order, Degree of Precision: 3

\section*{19: Newton-Cotes \& Composite}
\textbf{Newton-Cotes:} $\int_a^b f(x)dx \approx \sum_{i=0}^n w_if(x_i)$, $w_i = \int_a^b L_i(x)dx$ \\
- Closed: Includes endpoints (e.g., Trapezoidal, Simpson’s) \\
- Open: Excludes endpoints (e.g., Midpoint: $2hf(x_1) + \frac{h^3}{3}f''(\xi)$) \\
\textbf{Composite Trapezoidal:} \\
- $\int_a^b f(x)dx = \frac{h}{2}\left(f(a) + 2\sum_{i=1}^{n-1}f(x_i) + f(b)\right) - \frac{h^2}{12}(b-a)f''(\zeta)$ \\
\textbf{Composite Simpson’s:} ($n$ even) \\
- $\int_a^b f(x)dx = \frac{h}{3}\left(f(a) + 2\sum_{i=1}^{n/2-1}f(x_{2i}) + 4\sum_{i=1}^{n/2}f(x_{2i-1}) + f(b)\right) - \frac{h^4}{180}(b-a)f^{(4)}(\zeta)$ \\
\textbf{Ex:} $\int_0^2 \frac{1}{x+4}dx$, Error $< 10^{-5}$: Trap. $n \geq 46$, Simp. $n \geq 6$.

\section*{20: Stability \& Gaussian}
\textbf{Round-Off Stability:} \\
- Composite Simpson’s: $e(h) \leq (b-a)\varepsilon$, stable as $h \to 0$ \\
\textbf{Gaussian Quadrature:} $\int_a^b f(x)dx \approx \sum_{i=1}^n w_if(x_i)$ \\
- Exact for $P_{2n-1}$, nodes = Legendre roots \\
- $n=2$: $\int_{-1}^1 f(x)dx \approx f\left(-\frac{\sqrt{3}}{3}\right) + f\left(\frac{\sqrt{3}}{3}\right)$ \\
- $n=3$: $\int_{-1}^1 f(x)dx \approx \frac{5}{9}f\left(-\sqrt{\frac{3}{5}}\right) + \frac{8}{9}f(0) + \frac{5}{9}f\left(\sqrt{\frac{3}{5}}\right)$

\section*{21: Gaussian Quadrature}
\textbf{Arbitrary Interval:} $\int_a^b f(t)dt = \frac{b-a}{2} \int_{-1}^1 f\left(\frac{b-a}{2}x + \frac{b+a}{2}\right)dx$ \\
\textbf{Ex:} $\int_1^{1.6} \frac{2x}{x^2-4}dx \approx 0.733379$ ($n=3$) \\
\textbf{Legendre Polynomials:} $P_0=1$, $P_1=x$, $P_2=\frac{3x^2-1}{2}$, $P_3=\frac{5x^3-3x}{2}$ \\
\textbf{Error:} $\frac{(b-a)^{2n+1}(n!)^4}{(2n+1)[2n!]^3}f^{(2n)}(\zeta)$ \\
\textbf{Unbounded:} $\int_0^\infty e^{-x^2}dx = \int_0^1 e^{-\left(\frac{z}{1-z}\right)^2}\frac{1}{(1-z)^2}dz$

\section*{22: Linear Systems}
\textbf{Goal:} Solve $A\mathbf{x} = \mathbf{b}$, $A$ invertible \\
\textbf{Gaussian Elimination:} \\
- Transform to $U\mathbf{x} = \mathbf{y}$ (upper triangular) \\
- Backward Sub.: $x_n = \frac{y_n}{u_{nn}}$, $x_i = \frac{y_i - \sum_{j=i+1}^n u_{ij}x_j}{u_{ii}}$ \\
\textbf{Ex:} $x_1+x_2+3x_4=4$, etc. $\to \mathbf{x} = (-1, 2, 0, 1)^\top$

\section*{23: Gaussian Elimination I}
\textbf{Augmented Matrix:} $[A|\mathbf{b}]$ \\
\textbf{Algorithm:} \\
1. For $i=1$ to $n-1$: \\
   - Find $p$ s.t. $a_{pi} \neq 0$, swap $E_p \leftrightarrow E_i$ if needed \\
   - For $j=i+1$ to $n$: $E_j \leftarrow E_j - \frac{a_{ji}}{a_{ii}}E_i$ \\
2. Back-substitute \\
\textbf{Ex:} $x_1-x_2+2x_3-x_4=-8$, etc. $\to \mathbf{x} = (-7, 3, 2, 2)^\top$

\section*{24: Gaussian Elimination II}
\textbf{Cost:} \\
- Mult/Div: $\frac{n^3}{3} - n^2 - \frac{n}{3}$, Add/Sub: $\frac{n^3}{3} + \frac{n^2}{2} - \frac{5n}{6}$, $\mathcal{O}(n^3)$ \\
\textbf{Partial Pivoting:} Choose $p$ with max $|a_{pi}|$ \\
\textbf{Ex:} $0.003x_1+59.14x_2=59.17$, etc. $\to$ Pivot: $x_1=10$, $x_2=1$

\section*{25: LU Decomposition}
\textbf{LU:} $A = LU$, $L$ lower, $U$ upper \\
- Solve: $L\mathbf{y} = \mathbf{b}$, $U\mathbf{x} = \mathbf{y}$ \\
- Cost: $\mathcal{O}(n^3)$, Solve $\mathcal{O}(n^2)$ \\
- With swaps: $A = P^{-1}LU$, $P$ permutation \\
\textbf{Ex:} $A = \begin{bmatrix} 1 & 1 & 0 & 3 \\ 2 & 1 & -1 & 1 \\ 3 & -1 & -1 & 2 \\ -1 & 2 & 3 & -1 \end{bmatrix}$, $\mathbf{x} = (3, -1, 0, 2)^\top$

\section*{26: Special Matrices}
\textbf{Diag. Dominant:} $|a_{ii}| \geq \sum_{j \neq i} |a_{ij}|$, stable w/o swaps \\
\textbf{SPD:} $A^\top = A$, $\mathbf{x}^\top A\mathbf{x} > 0$ \\
- Cholesky: $A = LL^\top$, $l_{ii} = \sqrt{a_{ii} - \sum_{k=1}^{i-1} l_{ik}^2}$ \\
\textbf{Tridiagonal:} $A = LU$, Cost $\mathcal{O}(n)$ \\
\textbf{Ex:} $A = \begin{bmatrix} 4 & -1 & 1 \\ -1 & 4.25 & 2.75 \\ 1 & 2.75 & 3.5 \end{bmatrix}$, $L = \begin{bmatrix} 2 & 0 & 0 \\ -0.5 & 2 & 0 \\ 0.5 & 1.5 & 1 \end{bmatrix}$

\section*{27: Iterative Methods}
\textbf{Jacobi:} $x_i^{(k+1)} = \frac{1}{a_{ii}}\left(b_i - \sum_{j \neq i} a_{ij}x_j^{(k)}\right)$ \\
\textbf{Gauss-Seidel:} $x_i^{(k+1)} = \frac{1}{a_{ii}}\left(b_i - \sum_{j<i} a_{ij}x_j^{(k+1)} - \sum_{j>i} a_{ij}x_j^{(k)}\right)$ \\
\textbf{SOR:} $x^{(k+1)} = (1-\omega)x^{(k)} + \omega x_{\text{GS}}^{(k+1)}$, $\omega \in (0, 2)$ \\
\textbf{Convergence:} $\rho(B) < 1$ \\
\textbf{Conjugate Gradient:} For SPD, $\phi(x) = \frac{1}{2}x^\top Ax - x^\top b$ \\
- $x^{k+1} = x^k + \alpha_k s_k$, $\alpha_k = \frac{r_k^\top s_k}{s_k^\top A s_k}$ \\
\textbf{Ex:} $A_{5\times5}$, $\mathbf{x}^* \approx (7.86, 0.42, -0.07, -0.54, 0.01)^\top$

\end{multicols}
\end{document}
