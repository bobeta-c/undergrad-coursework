\documentclass[a4paper,12pt]{scrartcl} % From KOMA-script
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{lipsum} % For filler text
\usepackage{amsthm}
\usepackage{times}
\usepackage{microtype}
\usepackage{amsmath, amssymb, amsfonts, amssymb, float, enumitem}
\definecolor{darkbg}{HTML}{1E1E1E}
\definecolor{darktext}{HTML}{E0E0E0}
\definecolor{theoremcolor}{HTML}{d3dce5}
\definecolor{answercolor}{HTML}{e9dfc0}
\definecolor{sectioncolor}{HTML}{dec3c3}
\pagecolor{darkbg}
\color{darktext}
\newenvironment{solution}
  {\par\color{answercolor}\textbf{Solution:}\ }
  {\par}
\newenvironment{prf}
  {\par\textbf{Proof:}\ }
  {\par}

\newcounter{customcounter}
\newcommand{\setcustomcounter}[1]{\setcounter{customcounter}{#1}}

\newtheoremstyle{darktheorem}
  {\topsep}{\topsep}
  {\itshape\color{theoremcolor}}{}
  {\bfseries\color{theoremcolor}}{.}{.5em}{}
\theoremstyle{darktheorem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{topic}[theorem]{Topic}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[customcounter]{Exercise}
\newtheorem{announcement}[theorem]{Announcement}

\usepackage{fancyhdr}
\pagestyle{fancy}

% Clear default headers
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}

% Set Author and Date in the top left
\fancyhead[L]{\textcolor{darktext}{\small Asher Christian \\ 006-150-286 \\ \today }}

\begin{document}


\title{\color{sectioncolor}W1 CS M146}
\author{}
\date{}
\maketitle

% Apply fancyhdr on the title page
\thispagestyle{fancy}
\section{Perceptron and Logistic Regression}
\begin{announcement}
    HW 0 to be submitted mandatory submission but not to be graded.\\
    Online quiz mandatory due later this week as well as HW 1 release.
\end{announcement}
\[
y = f(x)
.\] 
deterministic or non-deterministic relationship between $y$ labels, $x$ features.\\
We care about performance on unseen data.
\[
    (x,y) \sim p
.\] 
$p$ has an unknown distribution.\\
Features space $X$, label space $Y$ and target function $f: X \rightarrow Y$ as well as functions $h \in H: X \rightarrow Y$
input is training data from a data-generating distribution $p: \{(x_1,y_1), (x_2,y_2),...,(x_n,y_n)\}$ $(x_i,y_i) $ independent identically distributed$ \sim p$
and we output $h \in H$ that best approximates $f$ in terms of loss function.
\begin{topic}
    Types of loss functions\\
    \begin{enumerate}
        \item square loss function when $y$ is continuous
            \[
                l(y,h(x)) = [h(x)-y]^2
            .\] 
        \item classification loss
            \[
                l(y,h(x)) = 1\{y \ne h(x)\}
            .\] 
            assumes $h$ outputs values in a finite set $Y$ where $|Y|$ are the number of classes.
        \item logistic loss
            \[
                l(y,h(x)) = -y \log h(x) - (1-y) \log [1-h(x)]
            .\] 
            assumes $h$ outputs values in $[0,1]$. 
            cross entropy - caputres the kullbock-leibler divergence which is a measure
            of "differences" in prob
    \end{enumerate}
    Components
    \begin{enumerate}
        \item Hpothesis class $H \rightarrow$ choose a fmaily of functions (neural networks, decision trees, SVM)
        \item Loss function $\rightarrow l(\hat y, y) : \hat Y \times Y \rightarrow \mathbb{R}^{+}$
        \item Criterion to find $h \in H$ to best predict from data
    \end{enumerate}
\end{topic}
\begin{topic}
    Measure how good our hypothesis $h$ is\\
    Asssume we know the true distribution of data $p(x,y)$ the risk is
    \[
        R[h(x)] = \sum_{x,y}^{}l(h(x),y)p(x,y) = \mathbb{E}_{x,y}[l(h(x),y)]
    .\] 
    if we did know the true distribution $p(x,y)$ we can try to find the best hypothesis 
    \[
        h^{\text{opt}}(x) = \arg\min_{h\in H}R[h(x)]
    .\] 
    \begin{definition}
        Given an observation $x$, the optimal Bayes classifier is given by:
         \[
             h^{\text{opt}}(x) = \arg \max_{y}p(y|x)
        .\] 
    \end{definition}
    If we do not have $p$ how do we optimize the $ \mathbb{E}[l(h(x),y)$
    \[
        R^{\text{EMP}}[h(x)] = \frac{1}{N}\sum_{n}^{}l(h(x_n),y_n)
    .\] 
    \[
        R^{\text{EMP}}[h(x)] \rightarrow R[h(x)]
    .\] 
    as $N \rightarrow \infty$ by law of large numbers
    Goal:
    \[
        \min_{h \in H}R^{\text{EMP}}(h)
    .\]  
    Strategy
    \[
        D = \{x_1,y_1)...,(x_n,y_n)\}
    .\] 
    empiracle risk minimization. 
\end{topic}
\begin{example}
    Dataset generated by a noisy sine function\\
    Goal: find a function to minimize $l_2$ loss.
    Suppose
    \[
    |D| = N = 10
    .\] 
    number of training examples. and the hypothesis is degree $M$ polynomials.
    hiugher degree polynomial - more complex features leads to better results in the training data
    but can lead to worse result in test data. Underfitting - too low degree, overfitting - too complex.
\end{example}
\begin{topic}
    Perceptron Learning - 1940s-1950s McCulloch-Pitts.
    \begin{enumerate}
        \item Instance (feature vectors): $x \in \mathbb{R}^{D}$ 
        \item Label: y \in \{-1,+1\}
        \item Model/Hoptothesis
            \[
                H = \Ph:H : X \rightarrow Y \hspace{1cm} h(x) = \text{sign}(\sum_{d=1}^{D}w_dx_d +b)
            .\] , 
        \item learning goal: $y = h(x)$ 
            \begin{enumerate}
                \item Learn $w_1,...,w_d, b$
            \end{enumerate}
        \item inpus $x \in \mathbb{R}^{d}, w \in \mathbb{R}^{D}, b \in \mathbb{R}$
            \[
            a = \sum_{d=1}^{D}w_dx_d + b = w^{T}x +b
            .\] 
            \[
            \hat y = sign(a)
            .\] 
            do empirical risk minimization
            \[
                \min_{w_1,...,w_d,b} \frac{1}{N}\sum_{}^{}l(y_i,h(x_i))
            .\] 
    \end{enumerate}
\end{topic}


\end{document}
