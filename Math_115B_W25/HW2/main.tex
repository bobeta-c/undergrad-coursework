\documentclass{article}
\usepackage{amsmath} % For math equations
\usepackage{amsfonts} % For math fonts
\usepackage{amssymb} % For math symbols
\usepackage{float}
\usepackage{enumitem}
\usepackage{graphicx}
\setlist[enumerate,1]{label=\arabic*.}
\setlist[enumerate,2]{label=\alph*.,itemindent=2em}

\title{HW 2 - 115B}
\author{Asher Christian 006-150-286}
\date{ 17.01.25}

\begin{document}
    \maketitle
    \section{Exercise 1}
    \emph{For each of the following vector spaces $V$ and each ordered basis
    $B$, find an explicit formula for each vector in the dual basis $B^{*}$.}\\
    \begin{enumerate}
        \item $V = k^{3}$, $B = \{ \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ 2 \\ 1 \end{pmatrix}, \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}  \}$ \\
            Let $E$ be the canonical basis then
            \[
                [I]_E^{B} =
                \begin{pmatrix} 
                    1 & 1 & 0\\
                    0 & 2 & 0 \\
                    1 & 1 & 1
                \end{pmatrix} 
                [I]_B^{E} =
                \begin{pmatrix} 
                    1 & -\frac{1}{2} & 0\\
                    0 & \frac{1}{2} & 0\\
                    -1 & 0 & 1
                \end{pmatrix} 
            .\] 
            The canonical dual basis is the column vectors $(1,0,0), (0,1,0), (0,0,1)$ with $f(v)$ corresponding to matrix multiplication
            Thus
            \[
                B^{*} = \{(1,-\frac{1}{2},0) , (0,\frac{1}{2},0), (-1,0,1) \}
            .\] 
        \item $V = k[x]_{\le 2}, B = \{1,x,x^2\}$
            Let $I$ denote the identity and $D$ denote the derivative operator which was shown previously to be linear.
            The dual basis can be defined as
            \[
                B^{*} = \{I, D, \frac{D}{2}\}
            .\] 
            with function defined as follows.\\ For any $f^{*} \in V^{*}, f \in V$
            $f^{*}(f) = (f^{*} \circ f)(0)$
    \end{enumerate}

    \section{Exercise 2}
    \emph{Define some $f \in ( \mathbb{R}^2)^{*} \;\; f \begin{pmatrix} x \\ y \end{pmatrix}  = 2x + y$ and
        a function $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ via the
        formula $T \begin{pmatrix} x \\ y \end{pmatrix}  = \begin{pmatrix} 3x+2y \\x \end{pmatrix} $
        \begin{enumerate}
            \item Compute $T^{*}(f)$.\\
                \[
                T^{*}(f)(v) = f(T(v)) = 2(3x+2y) + x = 7x+4y
                .\] 
            \item Compute $[T^{*}]_{\mathcal{E}^{*}}$, where $\mathcal{E}$ is the standard ordered basis
                for $ \mathbb{R}^2$ and $\mathcal{E}^{*} = \{e_1^{*},e_2^{*}\}$
                is the dual basis, explicitly by finding scalars $a,b,c,d$ such that
                $T^{*}(e_1^{*}) = ae_1^{*} + ce_2^{*}$ etc.
                \[
                T^{*} = 
                \begin{pmatrix} 
                    3 & 1\\
                    2 & 0
                \end{pmatrix} 
                .\] 
            \item Compute $[T]_\mathcal{E}$ and $(T_\mathcal{E})^{t}$
                \[
                    [T]_\mathcal{E} = 
                \begin{pmatrix} 
                    3 & 2\\
                    1 & 0
                \end{pmatrix} 
                [T]_\mathcal{E}^{t}
                =
                \begin{pmatrix} 
                    3 & 1\\
                    2 & 0
                \end{pmatrix} 
                .\] 
        \end{enumerate}
    }

    \section{Exercise 3}
    \emph{
        Let $V$ denote a finite dimensional $k$-vector space. For any subset $S \subset V$ 
        define the annihilator $S^{0}$ of $S$ as
        \[
            S^{0} := \{f \in V^{*}: f(x) = 0 \;\;\; \forall x \in S\}
        .\] 
        \begin{enumerate}
            \item Prove that $S^{0}$ is a subspace of $V^{*}$\\
                \emph{
                    To show taht $S^{0}$ first note that $S^{0} \subset V^{*}$ 
                    since every element of $S^{0}$ is an element of $V^{*}$.
                    Then note that $S^{0}$ contains the $0$ element $f \in V^{*} \rightarrow f(x) = 0 \forall x \in V$ is
                    clearly an element of $S^{0}$. Additionally consider any $f_1,f_2 \in S^{0}, c \in k$.
                    \[
                        (f_1+cf_2)(x) = f_1(x) + cf_2(x) = 0 + c*0  \;\; \forall x \in S \rightarrow (f_1+cf_2) \in S^{0}
                    .\] 
                }
            \item If $W$ is a subspace of $V$ and $x \not\in W,$ prove that there exists some $f \in W^{0}$ such
                that $f(x) \ne 0$
                \emph{
                    Since $x \not\in W$ $x \ne 0$. Since $V$ is finite dimensional pick a basis with $x$ as an element $B = {x, v_1, v_2,...}$ 
                    And consider further the linear functional $f$  defined on $B$ s.t.
                    \[
                    f(v_i) = 
                    \begin{cases}
                        1 & v_i = x\\
                        0 & v_i \ne x
                    \end{cases}
                    .\] 
                    Since $x \not\in W$ any vector $w \in W$ written in $B$ coordinates has $0$ as the $x$ coefficient and so $f(w) = 0$ satisfying
                    the definition of $S^{0}$ and so $f \in S^{0}$ but $f(x) = 1 \ne 0$.
                }
            \item In class, we constructed an isomorphism $\phi: V \rightarrow V^{**}$. Prove that
                $(S^{0})^{0} = $ span$(\phi(S))$ where $\phi(S) := \{\phi(s): s \in S\}$ \\
                \emph{
                    First note that $\phi(v) = \lambda_v$ defined as $\lambda_v(f) = f(v) \;\; \forall f \in V^{*}, v \in V$
                     To show the equality we must show that for each $\lambda \in \phi(S), \lambda \in (S^{0})^{0}$ and vice-versa
                     Recall the definition of $(S^{0})^{0}$ 
                     \[
                         (S^{0})^{0} = \{\lambda \in V^{**}: \lambda(f) = 0 \;\;\; \forall f \in S^{0} \}
                     .\] 
                     Consider any $\lambda_v \in \phi(S)$ and the $v \in V$ corresponding to this function. clearly
                     $v \in S$. Consider additionally any arbitrary $f \in S^{0}$. $\lambda_v(f) = f(v)$ recalling the definition of
                     $S^{0}$, since $f \in S^{0}$ $f(v) = 0$ since $v \in S$ therefore  $\lambda_v$ satisfies the condition of $(S^{0})^{0}$.
                     This proves the first direction.\\
                     For the second direction pick a basis $B' = \{s_1,s_2,...,s_n\}$ for $S$ and since $S \subset V$ pick a basis $B$ for $V$ consisting of all elements of $B'$ 
                     and additional elements $B = \{s_1,s_2,...,s_n,v_1,v_2,...,v_m\}$.
                     Pick any $\lambda \in (S^{0})^{0}$ and assume for contradiction that $\lambda \not\in span(\phi(S))$. Use the isomorphism to get $v = \phi^{-1}(\lambda)$.
                     Using our basis 
                     \[
                     v = \alpha_1s_1 + \alpha_2s_2 + ... + \alpha_ns_n + \beta_1v_1+\beta_2v_2+...+\beta_mv_m
                     .\] 
                     and so
                     \[
                     \lambda(f) = \alpha_1f(s_1) + ... + \alpha_nf(s_n) + \beta_1f(v_1) + ... + \beta_mf(v_m)
                     .\] 
                     But since $\lambda(f^{0}) = 0$ whenever $f^{0} \in S^{0}$ each $ \beta_i = 0$ by the previous question
                     Thus:
                     \[
                         \lambda = \alpha_1\lambda_{s_1} + \alpha_2\lambda_{s_2} + ... + \alpha_n\lambda_{s_n}
                     .\] 
                     Which is a linear combination of elements of $\phi(S)$ and is thus in the span of $\phi(S)$.
                }
            \item For subspaces $W_1$ and $W_2$ of  $V$, prove that $W_1 = W_2$ iff $W_1^{0} = W_2^{0}$.\\
                \emph{
                    To prove the first direction assume first that $W_1^{0} = W_2^{0}$ and assume for the sake of contradiction
                    that $W_1 \ne W_2$ then WLOG there exists some $w \in W_1$ s.t. $w \not\in W_2$. But since $w \not\in W_2$ there exists 
                    some $f \in W_2^{0} = W_1^{0}$ s.t. $f(w) \ne 0$. However this is a contradiction by the definition of $W_1^{0}$ since
                    $f(w) = 0$ by definition of $w \in W_1^{0}$. This proves the first direction. The other direction is trivial since in the definition
                    of the two sets we can interchange $W_1$ and $W_2$.
                }
            \item For subspaces $W_1$ and $W_2$ prove that $(W_1+W_2)^{0} = W_1^{0} \cap W_2^{0}$
                \emph{
                    Consider the definition
                    \[
                        (W_1+W_2)^{0} := \{f \in V^{*}: f(x) = 0 \;\; \forall x \in (W_1+W_2)\}
                    .\] 
                    Consider any $f \in (W_1+W_2)^{0}$ :\\
                    $f \in V^{*}$, $f(w_1) = 0, f(w_2) = 0 \;\; \forall w_1 \in W_1, w_2 \in W_2$. Therefore $f \in W_1^{0}$ and $f \in W_2^{0}$ so $f \in W_1^{0}\cap W_2^{0}$:\\
                    Consider any $f \in W_1^{0} \cap W_2^{0}$:\\
                    $f \in V^{*}$, $f(w_1) = 0 \; \forall w_1 \in W_1$ since $f \in W_1^{0}$ and $f(w_2) = 0 \; \forall w_2 \in W_2$ since $f \in W_2^{0}$ 
                    therefor for any $w \in (W_1+W_2), w = \alpha w_1 + \beta w_2$ for some $\alpha,\beta \in k, \; w_1 \in W_1, w_2 \in W_2$ and 
                    $f(w) = f(\alpha w_1 + \beta w_2) = \alpha f(w_1) + \beta f(w_2) = \alpha * 0 + \beta * 0 = 0$ so $f \in (W_1+W_2)^{0}$
                }
        \end{enumerate}
    }
    \section{Exercise 4}
    \emph{
        Prove that if $W$ is a subspace of $V$, then $dim(W) + dim(W^{0}) = dim(V)$.
    }
    There are 2 cases either $V$ is finite dimensional or not. In the first case since $V$ is f.d. so is $W$. let $dim(V) = n, dim(W) = k$.
    Consider an ordered basis $B' = \{w_1,...,w_k\}$ of $W$ and extend it to an ordered basis $B = \{w_1,...,w_k,...,w_n\}$ of $V$. We know
    that for any $f \in V^{*}, f = \alpha_1w_1^{*} + \alpha_2w_2^{*} + ... + \alpha_kw_k^{*} + ... + \alpha_nw_n^{*}$ Consider any $f \in W^{0}$.
    then
    \[
    f(w_i) =
    \begin{cases}
        0 & i \in \{1,...,k\}\\
        c_i & i \in \{k+1,...,n\}
    \end{cases}
    .\] 
    For this to hold given the representation above each $\alpha_i = 0, \; \forall i \in \{1,...,k\}$
    Therefore every $f^{0} \in W^{0}$ can be represented as 
    \[
        f^{0} = \alpha_{k+1}w_{k+1}^{*} + ... \alpha_nw_n^{*}
    .\] 
    and so $\{w_{k+1}^{*}, ..., w_n^{*}\}$ form a basis for $W^{0}$ and $dim(W^{0}) = n-k$ so $k + n-k = n$ is true and we have proved the question.\\
    In the case $dim(V)$ is not finite. we must show that at least one of $dim(W), dim(W^{0})$ is not finite. There are two cases
    $dim(W)$ is finite or not. If it is not finite we are done. Assume $dim(W)$ is finite. Since $dim(V)$  is not finite $dim(V^{*})$ is not
    finite. Assume for contradiction that $dim(W^{0})$ is finite then pick a basis for $W^{0}$
    \[
        B^{0} = \{w_1^{*}, w_2^{*},...,w_l^{*}\}
    .\] 
    Additionally pick $l+1$ linearly independent vectors in $V \setminus W$  which is possible since $W$ is finite and $V$ infinite.
    \[
        \{v_1,...,v_{l+1}\}
    .\] 
    and the corresponding linear functionals  
    \[
        \{v_1^{*},...,v_{l+1}^{*}\}
    .\] 
    defined in the usual way. Clearly each of these linear functionals is in $W^{0}$ and each is linearly independent.
    But this leads to a contradiction because there cannot exist more than $l$ linearly independent linear functionals in $W_0$ if the basis was defined with $l$ elements. Thus
    a contradiction showing that indeed $W^{0}$ must be infinite.
    \section{Exercise 5}
    \emph{
        Suppose that $W$ is a finite dimensional vector space and $T: V \rightarrow W$ is a linear
    transformation. Prove that $ker(T^{*}) = R(T)^{0}$
    }\\
    Consider any $f \in ker(T^{*})$ Then clearly
    \[
    f(T(v)) = 0
    .\] 
    for all $v \in V$ and in particular this implies for any  $w \in R(T)$, $f(w) = 0$ since  $w \in R(T)$ is equivalent to
    $w = T(v)$ for some $v \in V$.  $f(w) = 0$ for all $w \in R(T)$ satisfies the same requirements that put $f \in R(T)^{0}$ 
    Thus $ker(T^{*}) \subset R(T)^{0}$.
    Consider any $f \in R(T)^{0}$ then for any $w \in R(T)$, $f(w) = 0$. In particular for any $v \in V, T(v) \in R(T)$  so $T(v) = w$ for some $w \in R(T)$ and $f(w) = 0$ by definition
    so $f(T(v)) = 0 \forall v \in V$ and  $f \in ker(T^{*})$ thus $R(T)^{0} \subset ker(T^{*})$ 

    \section{Exercise 6}
    \emph{
        Let $R$ denote the $3 \times 3$ real matrix $\begin{pmatrix} -3 & -3 & -4\\
        2 & 2 & 4\\
    0 & 0 & -1\end{pmatrix} $. Find all eigenvalues of $R$. For 
    each eigenvalue, compute the corresponding eigenspace.
    }\\
    Each eigenvalue is one such that $det(T-\lambda I) = 0$ 
    first
    \[
    det( \begin{pmatrix} 
        -3-\lambda & -3 & -4\\
        2 & 2-\lambda & 4\\
        0 & 0 & -1 - \lambda
    \end{pmatrix} 
    = -(\lambda + 1)((\lambda-2)(\lambda+3)+6) = -\lambda(\lambda+1)^2
    .\] 
    Clearly $\lambda = -1$ and $\lambda = 0$ are the only eigenvalues of $R$
    First to find all eigenvectors of eigenvalue 1 is to find all solutions to
    $Rv = 0$ These come in the form  $ \begin{pmatrix} a \\ -a \\ 0 \end{pmatrix} $ for any $a \in \mathbb{R}$ which is found by 
    row reducing $R$ thus the eigenspace of eigenvalue 0 is all vectors of the previous form.
    To find the eigenspace of eigenvalue -1 solve $(R+I)v = 0$ The solutions to this equation
    are of the form $\begin{pmatrix} -1.5b - 2c \\ b \\ c \end{pmatrix} $ for any $b,c \in \mathbb{R}$ And so the
    eigenspace of eigenvalue $-1$ is spanned by
    \[
        \{ \begin{pmatrix} -1.5 \\ 1 \\ 0 \end{pmatrix} , \begin{pmatrix} -2 \\ 0 \\ 1 \end{pmatrix} \}
    .\] 
    .
    \section{Exercise 7}
    \emph{
        For the linear transformation $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$, defined by the formula
        $T \begin{pmatrix} x \\ y \end{pmatrix}  = \begin{pmatrix} 4x-y \\ 2x+y \end{pmatrix} $ 
        , find a basis $B$ of $ \mathbb{R}^2$ s.t. $[T]_B$ is diagnoal
    }
    Consider 
    \[
        [T]_\mathcal{E} = \begin{pmatrix} 4 & -1\\ 2 & 1 \end{pmatrix} 
    .\] 
    First find the eigenvalues of $[T]_\mathcal{E}$
    \[
    det(T-\lambda I) = det
    \begin{pmatrix}  4-\lambda & -1\\
    2 & 1 -\lambda\end{pmatrix} = (4-\lambda)(1-\lambda)+2 = (\lambda-2)(\lambda-3)
    .\] 
    So 3 and 2 are eigenvalues. with eigenvectors 
    \[
        B := \{ \begin{pmatrix} 1 \\ 1 \end{pmatrix} , \begin{pmatrix} 1 \\ 2 \end{pmatrix} \}
    .\] 
    respectively
    picking those two vectors as the basis in order we note that 
    \[
        [T]_B = \begin{pmatrix} 
        3 & 0\\
        0 & 2\end{pmatrix} 
    .\] 
    Diagonal.
    \section{Exercise 8}
    \emph{
        Given some vector space $V$ and a linear endomorphism $T: V \rightarrow V$ 
        we define a $T$-invariant subspace of $V$ to be a subspace $W \subset V$ s.t.
        $T(W) \subset W$. For the following determine if $W$ is $T$-invariant subspace of $V$
    }
    \begin{enumerate}
        \item $V = \mathbb{R}[x], T(f(x)) = f'(x), W = R[x]_{\le 2}$
            Yes since for an arbitrary polynomial $ax^{n}$ $f'(x) = anx^{n-1}$
            so the degree can only decrease and will therefore stay less than or equal to 2.
        \item $V = R[x], T(f(x)) = xf(x), W = R[x]_{\le 2}$
            No consider  $x^2 \in W$. $T(x^2) = x^{3}$ which has degree 3 and is not in $W$
        \item  $V = k^{3}, T( \begin{pmatrix} x_1\\x_2\\x_3 \end{pmatrix} ) = \begin{pmatrix} x_1+x_2+x_3\\x_1+x_2+x_3\\x_1+x_2+x_3 \end{pmatrix} ,
            W = \{\begin{pmatrix} x_1\\x_2\\x_3 \end{pmatrix} : x_1=x_2=x_3\}$\\
            Yes since for any $w \in W, w = \begin{pmatrix} x_1 \\ x_1 \\ x_1 \end{pmatrix} $ and 
            $T(w) = \begin{pmatrix} 3x_1 \\ 3x_1 \\ 3x_1 \end{pmatrix}  \in W$ 
        \item $V = C([0,1]) $ $T(f(t)) = (\int_{0}^{1}f(x)dx)t$ and $W = \{f \in V: f(t) = at+b, \;\; a,b \in \mathbb{R}\}$ 
            for every $f \in W$ since $f$ is continuous the integral exists and is a real number and so
            $T(f) = at$ with $b = 0$ in the definition and so is still in $W$
        \item  $V = k^{2 \times 2}, T(A) = \begin{pmatrix} 0 & 1\\ 1 & 0 \end{pmatrix} A$, $W$ is the subspace of symmetric matrices.\\
            No for consider an arbitrary $A \in W$ 
            \[
                A = \begin{pmatrix} a & b \\ b & c\end{pmatrix} 
            .\] 
            \[
                \begin{pmatrix} 0 & 1\\ 1 & 0 \end{pmatrix} A = \begin{pmatrix} b & c \\ a & b \end{pmatrix} 
            .\] 
            which is not symmetric unless $a = c$
    \end{enumerate}

    
    


\end{document}
