\documentclass{article}
\usepackage{amsmath} % For math equations
\usepackage{amsfonts} % For math fonts
\usepackage{amssymb} % For math symbols
\usepackage{float}
\usepackage{enumitem}
\usepackage{graphicx}
\setlist[enumerate,1]{label=\arabic*.}
\setlist[enumerate,2]{label=\alph*.,itemindent=2em}
\DeclareMathOperator{\Ima}{Im}
\DeclareMathOperator{\tr}{Tr}

\title{HW 5 - 115B}
\author{Asher Christian 006-150-286}
\date{ 10.02.25}

\begin{document}
    \maketitle
    \section{Exercise 1}
    \emph{
        Give an example of an inner product space $V$ and a linear operator $T: V \rightarrow V$ such
        that $\ker(T)$ and $\ker(T^{*})$ are not equal.
    }
    Consider $V = \mathbb{R}^{2}$ and $T = L_A$
    \[
        A =  \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} 
    .\] 
    and standard inner product.
    then
    \[
        \ker(T) = \text{span}\{ \begin{pmatrix} 1 \\ 0 \end{pmatrix} \}
    .\] 
    and
    \[
        [T^{*}]_B = \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix} 
    .\] 
    with
    \[
        \ker(T^{*})  = \text{span}\{\begin{pmatrix} 0 \\ 1 \end{pmatrix} \}
    .\] 
    thus the kernels are not equal

    \section{Exercise 2}
    \emph{
        Let $V$ be a finite dimensional inner product space, and let $W$ be a subspace
    }
    \begin{enumerate}
        \item \emph{ Prove $V = W \oplus W^{\perp}$ } \\
            Pick an orthogonal basis $B'$ for $W$,  $B' = \{v_1,v_2,...,v_n\}$. Extend this basis
            to an orthogonal basis $B$ for $V$, $B = \{v_1,v_2,...,v_n,v_{n+1},...,v_m\}$ This can be done by first picking a basis then using
            the graham schmidt process. Additionally $\{v_{n+1},...,v_m\}$ is a basis for $W^{\perp}$ this is because consider any element $w \in W^{\perp}$ then
            $w = \sum_{i=1}^{m}\alpha_iv_i$ but since $w \in W^{\perp}$, $w$ must be orthogonal to every element in $W$ that is $\langle w, v_j \rangle = \langle \sum_{i=1}^{m}\alpha_iv_i, v_j \rangle = \sum_{i=1}^{m} \alpha_i\langle v_i, v_j \rangle = 0$ if $j \le n$ 
            thus $\alpha_i = 0$ for all $i \le n$ Then for any $v \in V$,  $v = \sum_{i=1}^{m}\alpha_i v_i$.
            Let $w_1 = \sum_{i=1}^{n}\alpha_i v_i$ and $w_2 = \sum_{i=n+1}^{m}\alpha_i v_i$
            Then clearly $w_1 \in W$, $w_2 \in W^{\perp}$ because $\langle w_1 , w_2 \rangle$ = $\sum_{i=1}^{n}\beta_i(\sum_{j=n+1}^{m}\langle v_i,\gamma_j v_j \rangle) = 0 $ holds for any $v_1 \in W, v_2 \in W^{\perp}$.
            Additionally since $B$ is a basis this is the unique representation. Thus since $\text{Span}\{v_{n+1},...,v_m\}$ is  $W^{\perp}$ every element is a unique sum of elements
            in $W$ and $W^{\perp}$
        \item \emph{Show that if $T$ is a projection on $W$ along $W^{\perp}$, then $T = T^{*}$}\\
            Pick an orthonormal basis $B' = \{v_1,v_2,..,v_n\}$ of $W$ and extend to $B = \{v_{n+1},...,v_m\}$ of $V$ orthonormal, then
            if  $v = \sum_{i=1}^{m}\alpha_iv_i$ and $Tv = \sum_{i=1}^{n}\alpha_iv_i$. if $w = \sum_{i=1}^{m}\beta_i v_i$ then
            \[
            \langle Tv, w \rangle = \sum_{i=1}^{n} \langle \alpha_i v_i, w \rangle = \sum_{i=1}^{n}\sum_{j=1}^{m} \langle \alpha_i v_i, \beta_j v_j \rangle = \sum_{i=1}^{n} \alpha_i \overline{\beta_i}
            .\] 
            And similarly
            \[
            \langle v, Tw \rangle = \sum_{i=1}^{m} \langle \alpha_i v_i, \sum_{j=1}^{n} \beta_i v_i \rangle = \sum_{i=1}^{m}\sum_{j=1}^{n} \langle \alpha_i v_i, \beta_j v_j \rangle = \sum_{j=1}^{n} \alpha_j \overline{\beta_j}
            .\] 
            The key point to note is that the inner product of distinct basis elements is 0 whereas the inner product of similar elements is 1 by normality. and so
            \[
            \langle Tv, w \rangle = \langle v, Tw \rangle \implies T = T^{*}
            .\] 
    \end{enumerate}
    \section{Exercise 3}
    By Hw 4 Q 8 Part (b) this question is proven.

    \section{Exercise 4}
    For each lin op $T$ on an inner product space $V$, determine whether $T$ is normal,
    self-adjoint, or neither.
    \begin{enumerate}[label = (\alph*)]
        \item $V = \mathbb{R}^2$ standard inner product, $T(\begin{pmatrix} x \\y \end{pmatrix}  = \begin{pmatrix} 2x - 2y \\ -2x + 5y \end{pmatrix} $\\
            In the standard basis (which is orthonormal under standard inner product) we have
            \[
                [T]_\mathcal{E} = \begin{pmatrix} 2 & -2\\ -2 & 5 \end{pmatrix} \implies [T^{*}]_\mathcal{E} = \overline{[T]_\mathcal{E}^{t}} = \begin{pmatrix} 2 & -2 \\ -2 & 5 \end{pmatrix} = [T]_\mathcal{E}
            .\] 
                and since the basis is orthonormal we can freely go in between representations so $T = T^{*}$ and thus $T$ is self-adjoint and therefore normal.
        \item  $V = \mathbb{C}^2$ with standard inner product, $T(\begin{pmatrix} x \\ y \end{pmatrix} ) = \begin{pmatrix} 2x + iy \\ x + 2y \end{pmatrix} $ \\
            In the standardard basis (again orthonormal under complex standard inner product) we have
            \[
                [T]_\mathcal{E} = \begin{pmatrix} 2 & i \\ 1 & 2 \end{pmatrix} \implies [T^{*}]_\mathcal{E} = \begin{pmatrix} 2 & 1 \\ -i & 2 \end{pmatrix} 
            .\] 
            and
            \[
                [TT^{*}]_\mathcal{E} = \begin{pmatrix} 5 & 2 + 2i \\ 2 - 2i & 5 \end{pmatrix}  \;\;\; [T^{*}T]_\mathcal{E} = \begin{pmatrix} 5 & 2 + 2i \\ -2i + 2 & 5 \end{pmatrix} 
            .\] 
            Thus $T$ is normal but is not self-adjoint.
        \item $V = \mathbb{R}[x]_{\le 2}, T(f) = f'$ and $\langle f, g \rangle = \int_{0}^{1}f(x)g(x)dx$ \\
            Short computation yields an orthonormal basis using the gram-schmidt process on $\{1,x,x^2\}$ to
            get the orthonormal basis
            \[
            B = \{1, \sqrt{12}(x-\frac{1}{2}), 6\sqrt{5}(x^2-x+\frac{1}{6})
            .\] 
            and
            \[
                [I]^\mathcal{E}_{B} = \begin{pmatrix} 1 & -\sqrt{3} & \sqrt{5} \\
                    0 & \sqrt{12} & - 6\sqrt{5}\\
                    0 & 0 & 6\sqrt{5}
                \end{pmatrix} 
                \;\;\;
                [I]_\mathcal{E}^{B} = \begin{pmatrix} 
                    1 & \frac{1}{2} & \frac{1}{3}\\
                    0 & \frac{\sqrt{3}}{6} & \frac{\sqrt{3}}{6}\\
                    0 & 0 & \frac{\sqrt{5}}{30}
                \end{pmatrix} 
                [D]_\mathcal{E} = \begin{pmatrix} 
                    0 & 1 & 0\\
                    0 & 0 & 2\\
                    0 & 0 & 0
                \end{pmatrix} 
            .\] 
            Thus
            \[
                [D]_B = [I]_\mathcal{E}^{B}[D]_\mathcal{E}[I]_B^{\mathcal{E}} = \begin{pmatrix} 0 & \sqrt{3}/6 & -\frac{\sqrt{15}}{30} + \frac{\sqrt{3}}{6}\\
                    0 & 0 & \frac{\sqrt{15}}{15} \\
                    0 & 0 & 0
                \end{pmatrix} 
                    = \begin{pmatrix} 0 & a & b\\
                    0 & 0 & c \\
                    0 & 0 & 0
                    \end{pmatrix} 
            .\] 
            and
            \[
                [D]_B[D^{*}]_B = \begin{pmatrix}  a^2 + b^2 & ... \\ \vdots & \ddots\end{pmatrix} 
            .\] 
            but
            \[
                [D^{*}]_B[D]_b = \begin{pmatrix}  0 & ... \\ \vdots & \ddots \end{pmatrix} 
            .\] 
            so the operators are not the same and therefore the operator is not normal
        \item $V = \mathbb{R}^{2 \times 2}$ and $T(M) = M^{t}$ where $\langle A, B \rangle = tr(B^{*}A)$\\
            consider first comparing
            \[
            \langle A^{t}, B \rangle = tr(\overline{B^{t}}A^{t}) = tr(A\overline{B})
            .\] 
            and
            \[
            \langle A, B^{t} \rangle = tr(\overline{B}A)
            .\] 
            but for any $A, B \in \mathbb{R}^{2 \times 2}$ 
            \[
                tr(AB) = \sum_{i=1}^{n}(AB)_{ii} = \sum_{i=1}^{n}\sum_{j=1}^{n}A_{ij}B_{ji} = \sum_{j=1}^{n}\sum_{i=1}^{n}B_{ji}A_{ij} = \sum_{j=1}^{n}(BA)_{jj} = tr(BA)
            .\] 
            and so the two inner products are the same and thus $T = T^{*}$
    \end{enumerate}

    \section{Exercise 5}
    Let $T$ and $U$ be self-adjoint operators on an inner product space $V$. Prove that $TU$ is self-adjoint iff $TU = UT$.
     \[
    \langle TU v, w \rangle = \langle Uv , T^{*}w \rangle = \langle v, U^{*}T^{*}w \rangle
    .\] 
    similarly
    \[
    \langle UT v, w \rangle = \langle v, T^{*}U^{*}w \rangle
    .\]
    since this holds for any $v,w$ it must be the case that $TU = UT$ if and only if $U^{*}T^{*} = T^{*}U^{*}$ 

    \section{Exercise 6}
    \emph{
        Let $V$ be a complex inner product space, and let $T$ be a linear operator on $V$. Define
        \[
        T_1 = \frac{1}{2}(T + T^{*}) \;\;\;\; T_2 = \frac{1}{2i}(T-T^{*})
        .\] 
    }
    \begin{enumerate}[label = (\alph*)]
        \item \emph{Prove that $T_1$ and $T_2$ are self adjoint and that $T = T_1 + iT_2$}\\
            \begin{align*}
                \langle \frac{1}{2}(T +T^{*})v, w \rangle &= \frac{1}{2}(\langle Tv, w \rangle + \langle T^{*}v,w \rangle )\\
                                  &= \frac{1}{2}( \langle v, T^{*}w \rangle + \langle v, Tw \rangle )\\
                                  &=  \langle v, \frac{1}{2}(T^{*} + T)w \rangle
            \end{align*}
            similarly
            \begin{align*}
                \langle \frac{1}{2i} (T-T^{*})v, w \rangle &= \frac{1}{2i}( \langle Tv, w \rangle - \langle T^{*}v, w \rangle )\\
                                                           &= \frac{1}{2i}( \overline{\langle T^{*}w, v \rangle - \langle Tw, v \rangle} )\\
                                                           &= \overline{\frac{i}{2}  \langle (T^{*} - T)w, v \rangle}\\
                                                           &= \langle v, \frac{i}{2}(T^{*}- T)w \rangle\\
                                                           &= \langle v, \frac{1}{2i}(T - T^{*})w \rangle
            \end{align*}
            And so both operators are self adjoint
            by direct computation
            \[
            T_1 + iT_2 = \frac{1}{2}(T + T^{*}) + \frac{i}{2i}(T - T^{*}) = \frac{1}{2}(T + T^{*} + T - T^{*}) = \frac{1}{2}(2T) = T
            .\] 
            by linearity.
        \item \emph{ Suppose also that $T = U_1 + iU_2$ where $U_1$ and $U_2$ are self-adjoint prove that
            $U_1 = T_1$ and $U_2 = T_2$ }
            note that
            \begin{align*}
                \langle (U_1 + iU_2) v , w \rangle &= \overline{\langle U_1 w, v \rangle} + \overline{-i \langle U_2 w, v \rangle} = \langle v, (U_1 - i U_2 )w  \rangle
            \end{align*}
            Thus 
            \[
            T^{*} = U_1 -i U_2
            .\] 
            and so
            \[
            T_1 = \frac{1}{2}(T + T^{*}) = \frac{1}{2}(U_1 + iU_2 + U_1 - iU_2) = \frac{1}{2}(2U_1) = U_1
            .\] 
            and therefore $T_2 = U_2$ 
        \item \emph{Prove that $T$ is normal iff $T_1T_2 = T_2T_1$}
            \begin{align*}
                T_1T_2 &= \frac{1}{2}(TT_2 + T^{*}T_2)\\
                       &= \frac{1}{4i}(T^2 - TT^{*}+T^{*}T - (T^{*})^{2})\\
                T_2T_1 &= \frac{1}{2i}(TT_1 - T^{*}T_1)\\
                       &= \frac{1}{4i}(T^2 + TT^{*} - T^{*}T - (T^{*})^2)\\
            \end{align*}
            By direct computation if $T$ is normal then $TT^{*} = T^{*}T$ and so both equations are equal
            likewise if the two equations are equal then
             \[
            TT^{*} - T^{*}T =  - TT^{*} + T^{*}T \iff T^{*}T = TT^{*}
            .\] 
    \end{enumerate}

    \section{Exercise 7}
    \emph{
        Let $T$ be a linear operator on an inner product space $V$, and let $W$ be a $T$-invariant
        subspace of $V$. Prove the following
    }
    \begin{enumerate}[label = (\alph*)]
        \item \emph{If $T$ is self-adjoint, then $T|_W$ is self adjoint.}\\
            Let $v,w \in W$
            $Tv = T|_Wv$ and $Tw = T|_Ww$ and
             \[
            \langle T|_W v, w \rangle = \langle Tv, w \rangle = \langle v, Tw \rangle = \langle v, T|_Ww \rangle
            .\] 
            this holds for arbitrary $v,w$ and so $T|_W$ is self-adjoint.
        \item \emph{ $W^{\perp}$ is $T^{*}$-invariant}\\
            let  $w \in W^{\perp}$ and $v \in W$ then
            \[
            \langle v, T^{*}w \rangle =\langle Tv, w \rangle = \langle u, w \rangle
            .\] 
            for some  $u \in W$ since  $W$ is $T$-invariant, thus $\langle v, T^{*}w \rangle = 0$ for all $v \in W$
            and so  $T^{*}w \in W^{\perp}$ thus $W^{\perp}$ is $T^{*}$-invariant
        \item \emph{ If $W$ is both $T$- and $T^{*}$-invariant, then $(T|_W)^{*} = (T^{*})|_W$}
            If $W$ is both $T$ and $T^{*}$ invariant then
            for any $v,w \in W$ $Tv = T|_Wv$ and $T^{*}w = T^{*}|_w$ and likewise
            \[
            \langle T|_W v,w \rangle = \langle Tv, w \rangle = \langle v, T^{*}w \rangle = \langle v, T^{*}|_W w \rangle
            .\] 
        \item \emph{ If $W$ is both $T$ and $T^{*}$ invariant and $T$ is normal then $T|_W$ is normal}\\
           If $W$ is $T$ and $T^{*}$ invariant, then  by previous questions $(T|_W)^{*} = (T^{*})|_W$
           and so
           for any $w \in W$,  $T|_W(T^{*})|_Ww = TT^{*}w = T^{*}Tw = (T^{*})|_WT|_Ww$ and since this holds for all
           elements of $w$ the functions are equivalent and thus $T|_W$ is normal.
    \end{enumerate}
    \section{Exercise 8}
    \emph{
        Let $T$ be a normal operator on a finite-dimensional complex inner product space $V$, and let  $W$ be a subspace of $V$,
        prove that if  $W$ is $T$-invariant, then $W$ is also $T^{*}$-invariant.
    }
    First pick an orthonormal basis $B' = \{v_1,v_2,...,v_n\}$ for $W$ and extend it to $B = \{v_1,...,v_n,v_{n+1},...,v_m\}$ orthonormal basis of $V$ possible
    by gram schmitdt process.
    Then
     \[
         [T]_B = \begin{pmatrix} A_{1,1} & A_{1,2} \\ 0 & A_{2,2} \end{pmatrix} \;\;
         [T^{*}]_B = [T]_B^{*} = \begin{pmatrix} 
             A_{1,1}^{*} & 0\\
             A_{1,2}^{*} & A_{2,2}^{*}
         \end{pmatrix} 
    .\] 
    By $T$-invariance of $W$
    and likewise
    \[
        [TT^{*}]_B = [T]_B[T^{*}]_B = \begin{pmatrix} 
            A_{1,1}A^{*}_{1,1} + A_{1,2}A^{*}_{1,2} & ... \\
            \vdots & A_{2,2}A^{*}_{2,2}
        `\end{pmatrix} 
    .\] 
    and
    \[
        [T^{*}T]_B = [T^{*}]_B[T]_B = \begin{pmatrix} 
            A^{*}_{1,1}A_{1,1} & ...\\
            \vdots & \ddots
        \end{pmatrix} 
    .\] 
    Thus
    \[
        A_{1,1}A_{1,1}^{*} + A_{1,2}A_{1,2}^{*} = A^{*}_{1,1}A_{1,1}
    .\] 
    and likewise
    \[
        \tr(A_{1,1}A_{1,1}^{*}) + \tr(A_{1,2}A_{1,2}^{*}) = \tr(A^{*}_{1,1}A_{1,1})
    .\] 
    but $\tr(AA^{*}) = \tr(A^{*}A)$ by the following
    \[
        \tr(AA^{*}) = \sum_{i=1}^{n}(AA^{*})_{ii} = \sum_{i=1}^{n}\sum_{j=1}^{n}A_{ij}A^{*}_{ji} = \sum_{i=1}^{n}\sum_{j=1}^{n}A_{ij}\overline{A_{ij}} = \sum_{i=1}^{n}\sum_{j=1}^{n}\overline{A_{ij}}A_{ij}
    .\] 
    \[
        = \sum_{j=1}^{n}\sum_{i=1}^{n}A^{*}_{ji}A_{ij} = \sum_{j=1}^{n}(A^{*}A)_{jj} = \tr(A^{*}A)
    .\] 
    since
    \[
        (a+bi)(a-bi) = a^2 + b^2 = (a-bi)(a+bi)
    .\] 
    so
    \[
        \tr(A_{1,2}A^{*}_{1,2}) = 0
    .\] 
    but since
    \[
        A_{ij}\overline{A_{ij}} \ge 0
    .\] 
    this implies that each $(A_{1,2})_{ij} = 0$ for all $i,j$ in its dimension and thus  $A_{1,2} = 0$ and
     \[
         [T]_B = \begin{pmatrix} 
         A_{1,1} & 0\\
         0 & A_{2,2}
     \end{pmatrix} 
     [T^{*}]_B = \begin{pmatrix}
         A^{*}_{1,1} & 0\\
         0 & A^{*}_{2,2}
     \end{pmatrix} 
    .\] 
    from which it is evident that $W$ is $T^{*}$-invariant.
\end{document}
