\documentclass{article}
\usepackage{amsmath} % For math equations
\usepackage{amsfonts} % For math fonts
\usepackage{amssymb} % For math symbols
\usepackage{float}
\usepackage{enumitem}
\usepackage{graphicx}
\setlist[enumerate,1]{label=\arabic*.}
\setlist[enumerate,2]{label=\alph*.,itemindent=2em}
\DeclareMathOperator{\Ima}{Im}
\DeclareMathOperator{\tr}{Tr}

\title{HW 6 - 115B}
\author{Asher Christian 006-150-286}
\date{ 15.02.25}

\begin{document}
    \maketitle
    \section{Exercise 1}
    \emph{
        Let $T$ be a normal operator on a finite-dimensional real inner product space $V$ 
        whose characteristic polynomial splits. Prove that $V$ has an orthonormal basis
        of eigenvectors of $T$, and use this to prove $T$ is self adjoint.
    }
    Since $\chi_T(\lambda)$ (characteristic polynomial) splits over $ \mathbb{R}$ by the theorem presented
    in class there exists an orthonormal basis $B = \{v_1,v_2,...,v_n\}$ of $V$ such that
    $[T]_B$ is upper triangular. We then prove inductively on basis element $v_i$
    if $i=1$ then since $[T]_B$ upper triangular  $Tv_1 = \lambda_1 v_1 + 0v_2 + .. + 0v_n = \lambda_1 v_1$ is an eigenvector.
    Assume now that for $i \in\{1,2,3,...,d\}$,  $v_i = \lambda_i v_i$ then consider 
    \[
        Tv_{d+1} = \alpha_1 v_1 + ... + \alpha_{d}v_{d} + \alpha_{d+1}v_{d+1} + 0v_{d+2} + ... + 0 v_{n}
    .\] 
    and for any $j \in {1,...,d+1}$
    \[
        \alpha_j = \langle Tv_{d+1},v_j \rangle = \langle v_{d+1}, T^{*}(v_j) \rangle = \langle v_{d+1},\overline{\lambda_j}v_j \rangle = 0
    .\] 
    so
    \[
        Tv_{d+1} = \alpha_{d+1}v_{d+1} = \lambda_{d+1}v_{d+1}
    .\] 
    is an eigenvector thus $[T]_B$ is diagonal and each $v_i$ is an eigenvector orthonormal.
    additionally
    \[
        [T^{*}]_B = [T]_B^{*} = \overline{[T]_B^{t}} = [T]_B^{t} = [T]_B
    .\] 
    since every entry is real and the matrix is diagonal since each element is an eigenvector. Thus $[T^{*}]_B = [T]_B \implies T = T^{*}$ 
    and $T$ is self-adjoint

    \section{Exercise 2}
    \emph{
        Prove Corollary 2 to Theorem 6.18. That is, prove that if $T$ is a linear operator on a 
        finite dimensional complex inner product space $V$, then: $V$ has an orthonormal basis
        of eigenvectors of $T$ with corresponding eigenvalues of absolute value 1 iff $T$ 
        is unitary.
    }
    If $T$ is unitary then
    \[
    ||Tv|| = ||v||
    .\] for all $v$. Additionally $TT^{*} = T^{*}T = I$ so $T$ is normal and so there exists
    an orthonormal basis of eigenvectors $B = \{v_1,...,v_n\}$. By unitary $||v_i|| = ||Tv_i|| = ||\lambda_i v_i|| = |\lambda_i|||v_i||$ so
    $| \lambda_i| = 1$ for each $i$. Conversely if $V$ has an orthonormal basis $B = \{v_1,...,v_n\}$ of eigenvectors with eigenvalues of absolute value $1$ then
    $T(B) = \{\lambda_1v_1, ..., \lambda_nv_n\}$ and
    \[
    \langle \lambda_i v_i, \lambda_j v_j \rangle = \lambda_i \overline{\lambda_j} \langle v_i, v_j \rangle = 0
    .\] 
    if $i \ne j$ and
     \[
    ||\lambda_iv_i|| = |\lambda_i||v_i| = 1
    .\] 
    Thus $T(B)$ is an orthonormal basis and so $T$ is unitary.

    \section{Exercise 3}
    \emph{
        We saythat matrix $A \in \mathbb{C}^{n \times n}$ is unitarily equivalent to $B \in C^{n \times n}$ if
        there exists a unitary matrix $P \in C^{n \times n}$ such that $A = P^{-1}BP$. Prove that this is an equivalence relation
        on $ \mathbb{C}^{n \times n}$
    }
    Let $\sim$ denote the equivalence.
    First
    \[
    A = IAI \implies A \sim A 
    .\] 
    \[
    A = P^{-1}BP \implies B = PAP^{-1} = (P^{-1})^{-1}AP^{-1} \implies A \sim B \iff B \sim A
    .\] 
    \[
    A = P^{-1}BP \;\; B = Q^{-1}CQ \implies A = P^{-1}Q^{-1}CQP = (QP)^{-1}C(QP) \implies A \sim B \; B \sim C \implies A \sim C
    .\] 
    this follows since $I$ is unitary $Iv = v$. If $P$ unitary then $P^{-1} = P^{*}$ is unitary since 
    \[
    \langle P^{*}v, P^{*}v \rangle = \langle PP^{*}v, v \rangle = \langle v, v \rangle
    .\] 
    and if $P$ unitary and $Q$ unitary then
    \[
    \langle PQv, PQv \rangle = \langle Qv, Qv \rangle = \langle v, v \rangle
    .\] 
    \section{Exercise 4}
    \emph{
        Let $T$ be a normal operator on a finite-dimensional combplex inner product space $V$.
        Use the sepctral decomposition $\lambda_1T_1 + ... + \lambda_mT_m$ of $T$(where $\lambda_i \in \mathbb{C}$ ) to prove the following
        \begin{enumerate}[label = (\alph*)]
            \item if $g(t) \in \mathbb{C}[t]$ then $g(T) = \sum_{i=1}^{m}g(\lambda_i)T_i$
            \item If some positive power of $T$ is the zero transformation, then $T$ itself is the zero transformation
            \item Let $U$ be a linear operator on $V$.  Then $U$ commutes with $T$ iff $U$ commutes with each
                $T_i$ for all $i \in \{1,2,...,m\}$.
            \item There exists a normal operator  $U$ on $V$ such that $U^2 = T$.
            \item $T$ is invertible iff $\lambda_i \ne 0$ for all $i \in \{1,2,...,m\}$
            \item  $T$ is a projection iff every eigenvalue of $T$ is 1 or 0
            \item $T = -T^{*}$ iff every $\lambda_i$ is purely imaginary, that is, it lies in the set $i\mathbb{R} := \{ix : x \in \mathbb{R}\}$
        \end{enumerate}
    }
        \begin{enumerate}[label = (\alph*)]
            \item if $g(t) = a_0 + a_1t + ... + a_nt^{n} = \sum_{i=0}^{n}a_it^{i}$ then $g(T) = g(0)I + \sum_{i=1}^{n}a_i(T)^{i} = g(0)I + \sum_{i=1}^{n}a_i \sum_{j=1}^{m}\lambda_j^{i}T_j$
                This is because
                \[
                T^{i} = \lambda_1^{i}T_1 + ... + \lambda_m^{i}T_m
                .\] which we prove by induction on $m$ number of terms and $i$ power. if $m = i = 1$
                then
                \[
                T^{1} = \lambda_1T_1
                .\] 
                holds trivially
                and for all $i$ holding  $m$ at $1$
                 \[
                T= \lambda_1T_1 \rightarrow T^{i} = \lambda_1^{i}T_1^{i} = \lambda_1^{i}T_1
                .\] 
                by linearity of $T$ and the fact that we proved in previous homework that if $T$ projection then $T^2 = T$.
                assume it holds for $i,m$ then  for $i,m+1$
                \[
                    (\sum_{j=1}^{m+1}\lambda_jT_j)^{i} = (\lambda_{m+1}T_{m+1} + \sum_{j=1}^{m}\lambda_jT_j)^{i}
                .\] 
                \[
                    = \sum_{j=1}^{m}\lambda_j^{i}T_j + (\lambda_{m+1}T_{m+1})^{i} + \sum_{k}^{}\alpha_k(\lambda_{m+1}T_{m+1})^{s_k}(\sum_{j=1}^{m}\lambda_j^{r_k}T_j)
                .\] 
                where $\alpha_k$ are binomial coefficients and $s_k$ and $r_k$ are the powers associated with the coefficients
                But since

                \[
                    T_{i}T_{j} = \delta_{ij}T_i
                .\] 
                the last terms are all zero so it simplifies to
                \[
                    \sum_{j=1}^{m}\lambda_j^{i}T_j + \lambda_{m+1}^{i}T_{m+1} = \sum_{j=1}^{m+1}\lambda_j^{i}T_j
                .\] 
                Thus rewriting the first equation
                \[
                g(T) = g(0)I + \sum_{j=1}^{m}T_j\sum_{i=1}^{n}a_i\lambda_j^{i}
                .\] 
                but since $I = \sum_{j=1}^{m}T_j$ and $g(0) = a_0$
                \[
                = \sum_{j=1}^{m}T_ja_0 + \sum_{j=1}^{m}T_j\sum_{i=1}^{n}a_i\lambda_j^{i}
                .\] 
                \[
                = \sum_{j=1}^{m}T_j\sum_{i=0}^{n}a_i\lambda_j^{i}
                .\] 
                \[
                = \sum_{j=1}^{m}g(\lambda_j)T_j
                .\] 
            \item 
                \[
                T = \lambda_1T_1 + ... + \lambda_mT_m
                .\] 
                let $n$ be such that $T^{n} = 0$
                \[
                T^{n} = \lambda_1^{n}T_1 + ... + \lambda_m^{n}T_m = 0
                .\] 
                then for any $v = v_1 + ... + v_m$ with each $v_i$ such that $T_iv_j = \delta_{ij}v_j$
                \[
                T^{n}v = \lambda_1^{n}v_1 + ...  + \lambda_m^{n}v_m = 0
                .\] 
                since this holds for all $v$ there is a case where each $v_i \ne 0$ this implies each $\lambda_i^{n} = 0$ implies $\lambda_i = 0$ for all $i \in \{1,2,...,m\}$
                and thus  $T = 0$
            \item 
                first if $U$ commutes with each $T_i$
                \[
                    UT = U(\lambda_1T_1 + ... + \lambda_mT_m) = \lambda_1UT_1 + ... + \lambda_mUT_m = \lambda_1T_1U + ... + \lambda_mT_mU = TU
                .\] 
                proving the first direction. If instead $U$ commutes with $T$ then that implies
                for any $g(T), g \in \mathbb{C}[x]$ that $Ug(T) = g(T)U$ pick  $g_i(t)$ to be the lagrange polynomial
                such that $g_i(\lambda_j) = \delta_{ij}$ possible by lagrange interpolation. then by $(a)$,
                 \[
                g_i(T) = T_i
                .\] 
                and so
                \[
                Ug_i(T) = UT_i = g_i(T)U = T_iU
                .\] 
                Thus the other direction is proved.
            \item Let 
                \[
                U = \sqrt{\lambda_1}T_1 + ... + \sqrt{\lambda_m}T_m
                .\] 
                then
                \[
                U^2 = \lambda_1T_1 + ... + \lambda_mT_m = T
                .\] 
                by the proof for part (a)
            \item if each $\lambda_i \ne 0$ then let
                \[
                U = \sum_{i=1}^{m}\frac{1}{\lambda_i}T_i
                .\] 
                then
                \[
                UT = \sum_{i=1}^{m}\frac{1}{\lambda_i}T_iT = \sum_{i=1}^{m}\frac{1}{\lambda_i}T_i\lambda_iT_i
                .\] 
                this is because
                \[
                    T_iT = \sum_{j=1}^{m}T_i\lambda_jT_j = \sum_{j=1}^{m}\delta_{ij}T_i\lambda_j = \lambda_iT_i
                .\] 
                so
                \[
                UT = \sum_{i=1}^{m}T_i = I
                .\] 
                and since left inverse implies right inverse when dealing with endomorphism $TU = I$.
                This proves the first direction. To prove the second direction. To prove the second direction assume for contradiction that some $\lambda_i = 0$ then pick $v_i \in W_i$ eigenvector corresponding to the eigenspace  $W_i$.
                then if $T^{-1}$ is the inverse
                \[
                T^{-1}Tv_i = T^{-1}0 = 0 \ne v_i
                .\] 
                a contradiction and thus each $\lambda_i \ne 0$
            \item if each eigenvalue of $T$ is $1$ or $0$ then  if each $\lambda_i = 0$, $T = 0$ which is projection onto  $\{0\}$ along
                $V$ if not all are zero then wlog $\lambda_1 = 1$ and let $W_1$ be the eigenspace with eigenvalue $1$ then
                \[
                    T = T_1
                .\] 
                by spectral theorem and $T$ is a projection onto $W_1$ along $V - W_1 + \{0\}$
                if $T$ is a projection onto $W_1$ along $W_2$ then for each $v \in W_1$ $Tv  = v$ and for each  $w \ in W_2$ $Tw = 0$ and
                picking a basis $B = \{v_1,...,v_n\}$ of $W_1$ extending to a basis $B =\{v_1,...,v_n,w_1,...,v_m\}$ of $V$ then clearly 
                every basis element is an eigenvector of eigenvalue  $1$ or $0$ so every eigenvector is 1 or 0.
                additionally if $T = \sum_{i=1}^{m}\lambda_iT_i, T^2 = \sum_{i=1}^{m}\lambda_i^2T_i$ but if $\lambda_i \in \{0,1\}$ then $\lambda_i^2 = \lambda_i$ so they are equivalent and $T$ is a projection.
                Additionally if any $\lambda_i$ is not $0$ or $1$ then the two sides of the equation would be different and $T \ne T^2$ implies $T$ not a projection.
            \item If $T = -T^{*}$ then for every eigenvector
                \[
                Tv = \lambda v
                .\] 
                and
                \[
                T^{*}v = \overline{\lambda}v = -Tv = -\lambda v
                .\] 
                and since $v$ is not zero (an eigenvector)
                \[
                \overline{\lambda} = -\lambda
                .\] 
                implies that $\lambda$ has no real component. If instead every $\lambda_i$ is purely imaginary
                Since each eigenvector of  $T$ is an eigenvector of $T^{*}$ since $T$ is normal it is the case that whenever
                \[
                T = \sum_{i=1}^{m}\lambda_iT_i
                .\] 
                \[
                T^{*} = \sum_{i=1}^{m}\overline{\lambda_i}T_i
                .\] 
                so
                \[
                \sum_{i=1}^{m}\lambda_iT_i = \sum_{i=1}^{m}-\overline{\lambda_i}T_i
                .\] 
                and
                \[
                \sum_{i=1}^{m}(\lambda_i+\overline{\lambda_i})T_i = 0
                .\] 
                Since there exists some $v_i$ for each $T_i$ such that $T_iv_j = \delta_{ij}v_j$ each $T_i$ is linearly independent of eachother and
                the only way this sum can equal zero is if each $\lambda_i = -\overline{\lambda_i}$ which is only true if each $\lambda_i$ has only an imaginary component.
        \end{enumerate}

        \section{Exercise 5}
        \emph{
            Show that if $T$ is a normal operator on a complex finite-dimensional inner
            product space and $U$ is a linear operator that commutes with $T$, then $U$ also commutes with $T^{*}$
        }
        let
        \[
        T = \sum_{i=1}^{m}\lambda_iT_i
        .\] 
        be the spectral decomposition of $T$ guaranteed by normality over complex fdips. then $U$ commutes with each $T_i$
        additionally 
        \[
        T^{*} = \sum_{i_=1}^{m}\overline{\lambda_i}T_i
        .\] 
        and so
        \[
        UT^{*} = \sum_{i=1}^{m}\overline{\lambda_i}UT_i = \sum_{i=1}^{m}\overline{\lambda_I}T_iU = T^{*}U
        .\] 
        \section{Exercise 6}
        \emph{
            Let $T$ be a normal operator on a finite-dimensional inner product space. 
            Prove that if $T$ is a projection, then it is also an orthogonal projection.
        }
        Consider the eigenvectors of $T$ over $ \mathbb{C}$ for each one any eigenvector $v$ such that
        \[
        Tv = \lambda v
        .\] 
        since $T = T^2$
        \[
        T^2v = T\lambda v = \lambda^2 v = \lambda v
        .\] 
        implies
        \[
            \lambda^2 = \lambda
        .\] 
        which implies that $\lambda = 0$ or $\lambda = 1$ thus every eigenvalue is real and the characteristic polynomial splits over $ \mathbb{R}$.
        and everything the spectral theroem implies of $T$ holds regardless of the field.
        In particular
        \[
        T = \sum_{i=1}^{m}\lambda_iT_i \;\;\;\; T^{*} = \sum_{i=1}^{m}\overline{\lambda_i}T_i = \sum_{i=1}^{m}\lambda_iT_i
        .\] 
        and so $T$ is self adjoint  since each eigenvalue is real. and so
        $V = W_1 + W_2$ where $W_1 $ corresponds to eigenvalue $1$ and $W_2$ eigenvalue $0$.
        and by spectral theorem
        \[
        W_1 = W_2^{\perp} \;\;\; W_2 = W_1^{\perp}
        .\] 
        and so 
        $T = T_1$ 
        and $T$ is a projection onto $W_1$ along $W_1^{\perp}$ and is orthogonal.
        \section{Exercise 7}
        \emph{
            Let $U$ be a unitary operator on an inner product space $V$, and let $W$ be a finite
            dimensional $U$-invariant subspace of $V$. Prove that:
            \begin{enumerate}[label = (\alph*)]
                \item $U(W) = W$
                \item  $W^{\perp}$ is $U$-invariant.
            \end{enumerate}
        }
        \begin{enumerate}[label = (\alph*)]
            \item first $W$ is $U$-invariant so for every $w \in W$, $U(w) \in W$. let  $U|_W$ be the restriction of $U$ to $W$ and  $n$ be 
                the dimension of $W$. then by rank nullity
                \[
                    \dim(W) = \dim(\Ima(U|_W)) + \dim(\ker(U|_W))
                .\] 
                but $\ker(U|_W) = \{0\}$ since if it were not then there would exist a  $v \ne 0 \in W$ such that
                $Uv = 0$ but  $||Uv|| = ||v|| \ne 0 $ since $U$ unitary. so $\dim(\Ima(U|_W)) = n$ and it must equal all of $W$ 
                since it is an operator from $W$ to $W$. thus $U(W) = W$
            \item 
                Assume for contradiction that $W^{\perp}$ is not $U$-invariant, then for some $w \in W^{\perp}$ 
                $U(w) \not\in W^{\perp}$ then for some $v \in W$
                \[
                \langle  v, U(w) \rangle \ne 0
                .\] 
                but
                \[
                \langle v + w , v + w \rangle = \langle v,v \rangle + \langle v, w \rangle + \langle w, v \rangle + \langle w, w \rangle = \langle v, v\rangle + \langle w, w\rangle
                .\] 
                by orthogonality
                yet
                \[
                \langle U(v + w), U(v + w) \rangle = \langle U(v), U(v) \rangle + \langle U(v), U(w) \rangle + \langle U(w), U(v) \rangle + \langle U(w), U(w) \rangle
                .\] 
                thus
                \[
                \langle U(w), U(v) \rangle = - \langle U(v), U(w) \rangle = -\overline{\langle U(w), U(v) \rangle}
                .\] 
                so $\langle U(w), U(v) \rangle = bi$ is purely imaginary for every $v \in W$
                and since $U(W) = W$ we can rewrite the previous as
                \[
                \langle U(w), u \rangle = bi
                .\] 
                for any $u \in W$. Let  $u \in W$ be such that $b \ne 0$  which must be the case since we are assuming
                w perp is not U invariant and consider $iu$
                 \[
                \langle U(w), iu \rangle = -i \langle U(w), u \rangle = -ibi = b
                .\] 
                thus there exists some $v \in W$ such that
                \[
                \langle v+ w, v +w \rangle \ne \langle U(v+w), U(v+w) \rangle
                .\] 
                v being the inverse of $iu$ thus  $U$ is not unitary a contradiction so $W^{\perp}$ must be
                $U$-invariant.
        \end{enumerate}
        \section{Exercise 8}
        \emph{
            prove part (c) of the spectral theorem. In other words: assum that for a linear operator $T$ 
            on a finite dimensional inner product space $V$ which is normal if $F = \mathbb{C}$ 
            and self adjoint if $F = \mathbb{R}$ and let $W_1,...,W_m$ denote the eigenspaces
            corresponding to distinct eigenvalues of $T$ and let $T_i$ be the orthogonal projection
            of $V$ onto $W_i$. Then $T_iT_j = \delta_{ij}T_i$
        }
        Firstly note that each $W_i$ is disjoint except for the zero vector. Then if $i = j$, $T_iT_j = T_iT_i$ but since
        $T_i$ is a projection $T_i^2 = T_i$ so $T_iT_i = \delta_{ii}T_i$. If instead $i \ne j$ then for any $v = v_1 + v_2 +... + v_m$
        where each $v_i \in W_i$ 
         \[
        T_iT_j(v) = T_iT_jv_1 + T_iT_jv_2 + ... + T_iT_jv_m = 0 + 0 +... + T_iv_j + 0 +... + 0 = 0
        .\] 
        so for each $v$, $T_iT_j(v) = 0$ so $T_iT_j = 0 = \delta_{ij}$ when $i \ne j$
\end{document}
